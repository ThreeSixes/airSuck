AirSuck is an ADS-B/SSR processing and storage project that uses the dump1090 project to tune into and provide binary SSR data. It is designed to be used in a distributed fashion with support for multiple dump1090 instances as data sources, input data deduplication, MLAT pass-though, and employs REDIS for queueing and deduplication of data, as well as MongoDB for data storage. It should be possible to use some of the libraries classes such as ssrParse.py and cprMath.py with other Python projects, and connectors for multiple data inputs and outputs can be developed to expand supported data source applications and destination applications. This application is designed to be modular and to support operation across multiple hosts.

Future support will include ACARS data handling, and AIS as well.

File list:

"Daemons":
  - dump1090Connector.py - Handles connections to one or more dump1090 instances to recieve ADS-B Modes A, C, and S frames as hex strings with support for MLAT data. All data is passed through the ADS-B decoder and placed on a reliable queue to store raw frames and a pub/sub queue for further processing by the SSR state engine.
  - mongoDump.py - Stores incoming raw data from sources in a database for storage and reprocessing if necessary.
  - ssrStateEngine.py - Handles processing of stateful ADS-B data to build aircraft location data, call signs, etc. This process dumps aircraft state updates on a pub/sub queue for handling by other processes, and on a reliable queue for storage in MongoDB.
  - stateMongoDump.py - Stores state data in MongoDB for later processing.
  - node/stateNode.js - Node.js server for passing state JSON to a browser or other service. Requires Node.js and the following Node.js packages: redis, express, socket.io

Libraries:
  - ssrParse.py - Supports decoding of binary ADS-B data into relevant fields.
  - cprMath.py - Supports handling of Compact Position Reporting data.
  - airSuckUtil.py - Collection of tools for unit conversion, algorithms and functions for geographic data processing.

Clients:
  - sub2Dump1090.py - Feeds aggregated SSR data on the pub/sub queue from dump1090Connector.py and other sources back into dump1090 instances for testing purposes.
  - sub2Console.py - Dumps JSON strings generated by connector scripts on the feed pub/sub queue (such as dump1090Connector.py) to the Console.
  - sub2Parse.py - Feeds SSR data back through the SSR Parsing library and dumps the decoded frames to the console. This is mostly for debugging and development.
  - stateSub2Console.py - Feeds JSON strings generated by the state engines on the state pub/sub feed to the console.
  - stateSub2Loc.py - Displays updates from the state engine about vehicles that positioning data exists for.
  - stateSub2Geofence.py - Displays updates from the state engines about vehicles that have positioning data and are inside a configured radius around a configured GPS coordinate. This script is pre-configured for vehciles within 3 km of KPDX.

Test files:
  - sub2CrCTest.py - Checks CRC sums and performs XOR operations on frames. This was developed for testing.
  - ssrParseTest.py - Tests decoding of one or more manually entered frames by the ssrParse class. This was developed for testing.
  - cprMathTest.py - Class for testing Compact Position Reporting (CPR) algorithm. This was developed for testing.

Support config files:
  - supervisor/airSuck-dump1090Connector.conf - Supversior config file to keep dump1090Connector.py running as a daemon. 
  - supervisor/airSuck-mongoDump.conf - Supervisor config file to keep mongoDump.py running as a daemon.
  - supervisor/airSuck-ssrStateEngine.conf - Supervisor config file to keep ssrStateEngine.py running as a daemon.
  - supervisor/airSuck-stateMongoDump.conf - Supervisor config file to keep stateMongoDump.py running as a daemon.
  - supervisor/airSuck-stateNode.conf - Supervisor config file to keep node/stateNode.js running as a daemon.
  - The above files are all split out as individual config files to facilitate running some or all of these files on one or more servers. This makes it easier to split out roles in a multi-host environment.
